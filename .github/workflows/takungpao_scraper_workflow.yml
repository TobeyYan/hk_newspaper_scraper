name: Run Daily TaKungPao Scraper

on:
  workflow_dispatch: # Allows manual triggering
  schedule:
    # Runs every 5 hours, every day.
    # If a run hits timeout, the next scheduled run will pick up.
    - cron: '0 */6 * * *'

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: true

jobs:
  scrape_and_upload:
    runs-on: ubuntu-latest
    timeout-minutes: 300 # Set a timeout just under 5 hours (300 minutes).
                         # GitHub's default is 360 minutes (6 hours).

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Fetch history for checkpoint file if it's committed

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9' # Use the same Python version you developed with

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_takungpao.txt

    # Restore checkpoint file if it exists from previous runs
    - name: Restore Checkpoint
      uses: actions/cache@v4
      with:
        path: takungpao_checkpoint.txt
        key: ${{ runner.os }}-takungpao-checkpoint

    - name: Run TaKungPao Scraper
      env:
        BLOB_CONNECTION_STRING: ${{ secrets.BLOB_CONNECTION_STRING }}
        AZURE_CONTAINER_NAME: epaper
      run: |
        # Ensure the 'controllers' directory at the root is on the Python path
        export PYTHONPATH=$PYTHONPATH:$(pwd)
        python TaKungPao_scraper.py

    # Save checkpoint file after successful run
    # Use always() to ensure checkpoint is saved even if the job is cancelled or fails
    # due to the timeout, provided the 'python takungpao_scraper.py' step successfully
    # wrote the checkpoint file before being terminated.
    # However, if the Python script itself is terminated mid-day, it won't have
    # written the checkpoint for that day. The previous day's checkpoint will be loaded.
    - name: Save Checkpoint
      if: always() # Save even if cancelled, as long as the Python script had a chance to write
      uses: actions/cache/save@v4
      with:
        path: takungpao_checkpoint.txt
        key: ${{ runner.os }}-takungpao-checkpoint

    # Upload main scraper logs as an artifact
    - name: Upload Main Scraper Logs
      if: always() # Uploads even if the job fails or is cancelled
      uses: actions/upload-artifact@v4
      with:
        name: takungpao-scraper-main-log
        path: TaKungPao_scraper.log
        retention-days: 7

    # NEW STEP: Upload missing_pages.log as an artifact
    - name: Upload Missing Pages Log
      if: always() # Uploads even if the job fails or is cancelled
      uses: actions/upload-artifact@v4
      with:
        name: takungpao-missing-pages-log # A clear name for this artifact
        path: missing_pages.log           # The file generated by your script
        retention-days: 7                 # Optional: How long to keep this specific log file
